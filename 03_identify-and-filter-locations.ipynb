{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Identify and filter locations\n",
    "Look for location candidates in the extracted text using NLTK and Stanford NER, then filter location candidates based on rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eacheson\\Anaconda3\\envs\\pyscine\\lib\\site-packages\\nltk\\tag\\stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.tag import StanfordNERTagger\n",
    "# nltk customization\n",
    "nltk.data.path.append('C:\\\\Users\\\\eacheson\\\\Documents\\\\Data\\\\NLTK')\n",
    "StanfordBaseDir = 'C:\\\\Users\\\\eacheson\\\\Documents\\\\Java\\\\'\n",
    "os.environ['CLASSPATH'] = StanfordBaseDir + 'stanford-ner-2017-06-09\\\\'\n",
    "os.environ['STANFORD_MODELS'] = StanfordBaseDir + 'stanford-ner-2017-06-09\\\\classifiers'\n",
    "# nltk initialization of Stanford NER tagger\n",
    "tagger = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')\n",
    "\n",
    "# own files\n",
    "from pysci import docutils as du\n",
    "from pysci import geoparse as gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_pickle = 'science_articles.pkl'\n",
    "# if needed, re-serialize in addition to any CSV export\n",
    "path_to_repickle = 'science_articles_geoparsed.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the serialized ScienceDocs\n",
    "science_docs = du.load_data(path_to_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Processing article Liu_et_al-2015-Insect_Conservation_and_Diversity...\n",
      "### Found 1 location candidates in title:\n",
      "\tNorth China\n",
      "LOC final else: keep\n",
      "### Kept 1 location chunks in title:\n",
      "\tNorth China\n",
      "### Found 3 location candidates in sentence:\n",
      "\tXitiange\n",
      "\tNortheastern Miyun County (\n",
      "\tNCP\n",
      "LOC final else: keep\n",
      "LOC left of opening parenthesis: keep\n",
      "final else: discard\n",
      "### Kept 2 location chunks:\n",
      "\tXitiange\n",
      "\tNortheastern Miyun County (\n",
      "### Found 1 location candidates in sentence:\n",
      "\tBeijing city\n",
      "LOC final else: keep\n",
      "### Kept 1 location chunks:\n",
      "\tBeijing city\n",
      "### Found 1 location candidates in sentence:\n",
      "\tXitiange\n",
      "LOC final else: keep\n",
      "### Kept 1 location chunks:\n",
      "\tXitiange\n",
      "### Found 1 location candidates in sentence:\n",
      "\tWoodland\n",
      "LOC final else: keep\n",
      "### Kept 1 location chunks:\n",
      "\tWoodland\n",
      "\n",
      "### Processing article Russo_et_al-2013-Ecology_and_Evolution...\n",
      "### Found 2 location candidates in sentence:\n",
      "\tRussell E. Larson\n",
      "\tCentre County, PA (\n",
      "final else: discard\n",
      "chunk had a keyword: keep\n",
      "### Kept 1 location chunks:\n",
      "\tCentre County, PA (\n",
      "\n",
      "### Done.\n"
     ]
    }
   ],
   "source": [
    "# start from the ScienceDoc instances\n",
    "for scidoc in science_docs:\n",
    "    print(\"\\n### Processing article %s...\" %scidoc.file_name)\n",
    "\n",
    "    ### PARSE TITLE FROM XML ###\n",
    "    if scidoc.has_xml:\n",
    "        if scidoc.title:\n",
    "            scidoc.title_locations = []\n",
    "            title_string = scidoc.title\n",
    "            #print(\"Title: %s\" %title_string)\n",
    "            # process title\n",
    "            title_clean = gp.multireplace(title_string)\n",
    "            sent_tok = nltk.word_tokenize(title_clean)\n",
    "            sent_pos = nltk.pos_tag(sent_tok)\n",
    "            sent_ner = tagger.tag(sent_tok)\n",
    "            # customizable extract method\n",
    "            extracted_chunks = gp.extract_chunks_from_sentence(\n",
    "                sent_ner, include_cardinal=True, include_other_spatial=True, include_types=True)\n",
    "            if extracted_chunks:\n",
    "                print(\"### Found %s location candidates in title:\" %len(extracted_chunks))\n",
    "                for loc_chunk in extracted_chunks:\n",
    "                    loc_chunk_str = gp.tuple_list_to_string(loc_chunk)\n",
    "                    print(\"\\t%s\" %loc_chunk_str)\n",
    "                extracted_chunks_pos = gp.filter_chunk_candidates(sent_tok, extracted_chunks, verbose=True)\n",
    "                print(\"### Kept %s location chunks in title:\" %len(extracted_chunks_pos))\n",
    "                for loc_chunk_keep in extracted_chunks_pos:\n",
    "                    loc_chunk_keep_str = gp.tuple_list_to_string(loc_chunk_keep)\n",
    "                    # keep just the final filtered locations - empty list means we had none\n",
    "                    scidoc.title_locations.append(loc_chunk_keep_str)\n",
    "                    print(\"\\t%s\" %loc_chunk_keep_str)\n",
    "        else:\n",
    "            #print(\"No title for this article.\")\n",
    "            scidoc.title_locations = gp.NO_TITLE_STRING  \n",
    "    else:\n",
    "        #print(\"No xml file for this article.\")\n",
    "        scidoc.title_locations = gp.NO_XML_STRING\n",
    "\n",
    "    ### Process article contents\n",
    "    content_locations = []\n",
    "    content_locations_filtered = []\n",
    "    location_sentences = []\n",
    "    for par in re.split('[\\n]{2,}', scidoc.relevant_text):\n",
    "        par_clean = gp.multireplace(par)\n",
    "        #print(\"Clean paragraph: %s\" % (par_clean))\n",
    "        sentences = nltk.sent_tokenize(par_clean)\n",
    "        for sent in sentences:\n",
    "            sent_added = False\n",
    "            sent_tok = nltk.word_tokenize(sent)\n",
    "            sent_pos = nltk.pos_tag(sent_tok)\n",
    "            sent_ner = tagger.tag(sent_tok)\n",
    "            # customizable extract method\n",
    "            extracted_chunks = gp.extract_chunks_from_sentence(\n",
    "                sent_ner, include_cardinal=True, include_other_spatial=True, include_types=True)\n",
    "            if extracted_chunks:\n",
    "                #print(\"NER tagged sentence:\\n %s\" %sent_ner)\n",
    "                print(\"### Found %s location candidates in sentence:\" %len(extracted_chunks))\n",
    "                for loc_chunk in extracted_chunks:\n",
    "                    loc_chunk_str = gp.tuple_list_to_string(loc_chunk)\n",
    "                    content_locations.append(loc_chunk_str)\n",
    "                    print(\"\\t%s\" %loc_chunk_str)\n",
    "                extracted_chunks_pos = gp.filter_chunk_candidates(sent_tok, extracted_chunks, verbose=True)\n",
    "                print(\"### Kept %s location chunks:\" %len(extracted_chunks_pos))\n",
    "                for loc_chunk_keep in extracted_chunks_pos:\n",
    "                    loc_chunk_keep_str = gp.tuple_list_to_string(loc_chunk_keep)\n",
    "                    content_locations_filtered.append(loc_chunk_keep_str)\n",
    "                    print(\"\\t%s\" %loc_chunk_keep_str)\n",
    "                    if not sent_added:\n",
    "                        sent_no_breaks = sent.replace('\\n', ' ')\n",
    "                        location_sentences.append(sent_no_breaks)\n",
    "                        sent_added = True\n",
    "            \n",
    "    scidoc.content_locations = content_locations\n",
    "    scidoc.content_locations_filtered = content_locations_filtered\n",
    "    scidoc.location_sentences = location_sentences\n",
    "    \n",
    "print(\"\\n### Done.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickled data at science_articles_geoparsed.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optionally repickle article data with locations etc\n",
    "du.pickle_data(science_docs, path_to_repickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare per-article results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename_only</th>\n",
       "      <th>use_xml</th>\n",
       "      <th>title</th>\n",
       "      <th>title_locations</th>\n",
       "      <th>methods_sections</th>\n",
       "      <th>content_locations</th>\n",
       "      <th>content_locations_filtered</th>\n",
       "      <th>location_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Liu_et_al-2015-Insect_Conservation_and_Diversity</td>\n",
       "      <td>True</td>\n",
       "      <td>Effects of plant diversity, habitat and agricu...</td>\n",
       "      <td>North China</td>\n",
       "      <td>[Materials and methods, Study area and site se...</td>\n",
       "      <td>Xitiange; Northeastern Miyun County (; NCP; Be...</td>\n",
       "      <td>Xitiange; Northeastern Miyun County (; Beijing...</td>\n",
       "      <td>[This study was conducted at Xitiange village ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Russo_et_al-2013-Ecology_and_Evolution</td>\n",
       "      <td>False</td>\n",
       "      <td>Supporting crop pollinators with floral resour...</td>\n",
       "      <td></td>\n",
       "      <td>[Material and Methods]</td>\n",
       "      <td>Russell E. Larson; Centre County, PA (</td>\n",
       "      <td>Centre County, PA (</td>\n",
       "      <td>[We established floral provisioning habitat 25...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      filename_only  use_xml  \\\n",
       "0  Liu_et_al-2015-Insect_Conservation_and_Diversity     True   \n",
       "1            Russo_et_al-2013-Ecology_and_Evolution    False   \n",
       "\n",
       "                                               title title_locations  \\\n",
       "0  Effects of plant diversity, habitat and agricu...     North China   \n",
       "1  Supporting crop pollinators with floral resour...                   \n",
       "\n",
       "                                    methods_sections  \\\n",
       "0  [Materials and methods, Study area and site se...   \n",
       "1                             [Material and Methods]   \n",
       "\n",
       "                                   content_locations  \\\n",
       "0  Xitiange; Northeastern Miyun County (; NCP; Be...   \n",
       "1             Russell E. Larson; Centre County, PA (   \n",
       "\n",
       "                          content_locations_filtered  \\\n",
       "0  Xitiange; Northeastern Miyun County (; Beijing...   \n",
       "1                                Centre County, PA (   \n",
       "\n",
       "                                  location_sentences  \n",
       "0  [This study was conducted at Xitiange village ...  \n",
       "1  [We established floral provisioning habitat 25...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten each list entry to a string instead of a list\n",
    "filenames_for_df = []\n",
    "use_xml_for_df = []\n",
    "methods_for_df = []\n",
    "titles_for_df = []\n",
    "title_locations_for_df = []\n",
    "content_locations_for_df = []\n",
    "content_locations_filtered_for_df = []\n",
    "location_sentences_for_df = []\n",
    "for doc in science_docs:\n",
    "    filenames_for_df.append(doc.file_name)\n",
    "    use_xml_for_df.append(doc.use_xml)\n",
    "    try:\n",
    "        titles_for_df.append(doc.title)\n",
    "    except AttributeError:\n",
    "        titles_for_df.append(gp.NO_TITLE_STRING)\n",
    "    if doc.title_locations == gp.NO_XML_STRING or doc.title_locations == gp.NO_TITLE_STRING:\n",
    "        title_locations_for_df.append(doc.title_locations)\n",
    "    else:\n",
    "        title_locations_for_df.append('; '.join([x for x in doc.title_locations]))\n",
    "    try:\n",
    "        methods_for_df.append(doc.methods_sections)\n",
    "    except AttributeError:\n",
    "        methods_for_df.append('')\n",
    "    if not doc.content_locations:\n",
    "        content_locations_for_df.append('')\n",
    "    elif doc.content_locations == gp.NO_METHODS_STRING:\n",
    "        content_locations_for_df.append(doc.content_locations)\n",
    "    else:\n",
    "        content_locations_for_df.append('; '.join([x for x in doc.content_locations]))\n",
    "    if not doc.content_locations_filtered:\n",
    "        content_locations_filtered_for_df.append('')\n",
    "    elif doc.content_locations_filtered == gp.NO_METHODS_STRING:\n",
    "        content_locations_filtered_for_df.append(doc.content_locations_filtered)\n",
    "    else:\n",
    "        content_locations_filtered_for_df.append('; '.join([x for x in doc.content_locations_filtered]))\n",
    "    if not doc.location_sentences:\n",
    "        location_sentences_for_df.append('')\n",
    "    else:\n",
    "        location_sentences_for_df.append(doc.location_sentences)\n",
    "\n",
    "df_geoparsed = pd.DataFrame({'filename_only':filenames_for_df, \n",
    "                            'use_xml':use_xml_for_df,\n",
    "                            'title':titles_for_df,\n",
    "                            'title_locations':title_locations_for_df,\n",
    "                            'methods_sections':methods_for_df,\n",
    "                            'content_locations':content_locations_for_df,\n",
    "                            'content_locations_filtered':content_locations_filtered_for_df,\n",
    "                            'location_sentences':location_sentences_for_df})\n",
    "\n",
    "# increase the column width display of pandas tables to view full cells\n",
    "#pd.options.display.max_colwidth = 500\n",
    "\n",
    "df_geoparsed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geoparsed.to_csv(os.path.join('results', 'articles_geoparsed.tsv'), sep='\\t', index=False, quotechar='\"', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare per-location results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now each list item (each final content location) will be a separate row in the df\n",
    "filenames_flat = []\n",
    "content_locations_filtered_flat = []\n",
    "location_sentences_flat = []\n",
    "use_xml_flat = []\n",
    "for doc in science_docs:\n",
    "    if not doc.content_locations_filtered:\n",
    "        # store the 'no location' case!\n",
    "        content_locations_filtered_flat.append(gp.NO_LOCATIONS_STRING)\n",
    "        location_sentences_flat.append(gp.NO_LOCATIONS_STRING)\n",
    "        use_xml_flat.append(doc.use_xml)\n",
    "        filenames_flat.append(doc.file_name)\n",
    "        continue\n",
    "    elif doc.content_locations_filtered == gp.NO_METHODS_STRING:\n",
    "        content_locations_filtered_flat.append(gp.NO_METHODS_STRING)\n",
    "        location_sentences_flat.append(gp.NO_METHODS_STRING)\n",
    "        use_xml_flat.append(doc.use_xml)  # we store 'N/A' already\n",
    "        filenames_flat.append(doc.file_name)\n",
    "    else:\n",
    "        for location in doc.content_locations_filtered:\n",
    "            content_locations_filtered_flat.append(location)\n",
    "            use_xml_flat.append(doc.use_xml)\n",
    "            filenames_flat.append(doc.file_name)\n",
    "            found_sentence = False\n",
    "            for sentence in doc.location_sentences:\n",
    "                if location in sentence:\n",
    "                    found_sentence = True\n",
    "                    location_sentences_flat.append(sentence)\n",
    "                    break\n",
    "            if not found_sentence:\n",
    "                location_sentences_flat.append('no exact sentence match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>content_locations</th>\n",
       "      <th>use_xml</th>\n",
       "      <th>location_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Liu_et_al-2015-Insect_Conservation_and_Diversity</td>\n",
       "      <td>Xitiange</td>\n",
       "      <td>True</td>\n",
       "      <td>This study was conducted at Xitiange village i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Liu_et_al-2015-Insect_Conservation_and_Diversity</td>\n",
       "      <td>Northeastern Miyun County (</td>\n",
       "      <td>True</td>\n",
       "      <td>This study was conducted at Xitiange village i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Liu_et_al-2015-Insect_Conservation_and_Diversity</td>\n",
       "      <td>Beijing city</td>\n",
       "      <td>True</td>\n",
       "      <td>The area is situated about 70 km north of Beij...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Liu_et_al-2015-Insect_Conservation_and_Diversity</td>\n",
       "      <td>Xitiange</td>\n",
       "      <td>True</td>\n",
       "      <td>This study was conducted at Xitiange village i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Liu_et_al-2015-Insect_Conservation_and_Diversity</td>\n",
       "      <td>Woodland</td>\n",
       "      <td>True</td>\n",
       "      <td>Woodland was also planted with Populus spp., b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           filename  \\\n",
       "0  Liu_et_al-2015-Insect_Conservation_and_Diversity   \n",
       "1  Liu_et_al-2015-Insect_Conservation_and_Diversity   \n",
       "2  Liu_et_al-2015-Insect_Conservation_and_Diversity   \n",
       "3  Liu_et_al-2015-Insect_Conservation_and_Diversity   \n",
       "4  Liu_et_al-2015-Insect_Conservation_and_Diversity   \n",
       "\n",
       "             content_locations  use_xml  \\\n",
       "0                     Xitiange     True   \n",
       "1  Northeastern Miyun County (     True   \n",
       "2                 Beijing city     True   \n",
       "3                     Xitiange     True   \n",
       "4                     Woodland     True   \n",
       "\n",
       "                                  location_sentences  \n",
       "0  This study was conducted at Xitiange village i...  \n",
       "1  This study was conducted at Xitiange village i...  \n",
       "2  The area is situated about 70 km north of Beij...  \n",
       "3  This study was conducted at Xitiange village i...  \n",
       "4  Woodland was also planted with Populus spp., b...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flat = pd.DataFrame({'filename':filenames_flat,\n",
    "                        'content_locations':content_locations_filtered_flat,\n",
    "                        'use_xml':use_xml_flat,\n",
    "                        'location_sentences':location_sentences_flat})\n",
    "\n",
    "df_flat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat.to_csv(os.path.join('results', 'locations.tsv'), sep='\\t', index=False, quotechar='\"', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
