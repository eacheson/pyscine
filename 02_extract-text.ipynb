{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Extract text\n",
    "Read in the data from XML and txt files, create a data structure to work with, and look for relevant section headings, extracting relevant text from these sections along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# own files\n",
    "from pysci import docutils as du\n",
    "from pysci import geoparse as gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_dir = 'pdfs'\n",
    "corpus_name = 'test-corpus'\n",
    "path_to_pickle = 'science_articles.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section titles text:\n",
      "['Materials and methods', 'Study area and site selection']\n",
      "Relevant content text, length: 80\n",
      "Section titles xml:\n",
      "['Materials and methods', 'Study area and site selection']\n",
      "Relevant content xml, length: 1382\n",
      "Using XML: True\n",
      "Section titles text:\n",
      "['Material and Methods']\n",
      "Relevant content text, length: 522\n",
      "Section titles xml:\n",
      "['Material and Methods']\n",
      "Relevant content xml, length: 0\n",
      "Using XML: False\n",
      "We have 2 ScienceDocs.\n",
      "We have 2 pdf documents.\n",
      "We have 2 txt documents.\n",
      "We have 2 xml documents.\n"
     ]
    }
   ],
   "source": [
    "documents = []  # list of ScienceDocs\n",
    "min_characters = 100\n",
    "\n",
    "count_pdf_files = 0\n",
    "count_txt_files = 0\n",
    "count_xml_files = 0\n",
    "\n",
    "for root, dirs, files in os.walk(articles_dir):\n",
    "    # ignore files that aren't pdf\n",
    "    files[:] = [f for f in files if os.path.splitext(f)[1] == du.PDF_extension]\n",
    "    for filename_pdf in files:\n",
    "        count_pdf_files += 1\n",
    "        filepath_pdf = os.path.join(root,filename_pdf)\n",
    "        filename_raw = du.remove_extension(filename_pdf)\n",
    "        # create ScienceDoc object\n",
    "        scidoc = du.ScienceDoc(corpus_name=corpus_name, file_name=filename_raw)\n",
    "        documents.append(scidoc)\n",
    "        \n",
    "        ### PROCESS TXT ###\n",
    "        filename_txt = filename_raw + du.TXT_extension\n",
    "        filepath_txt = os.path.join(root,filename_txt)\n",
    "        if os.path.isfile(filepath_txt):\n",
    "            count_txt_files += 1\n",
    "            scidoc.has_text = True\n",
    "            with open(filepath_txt, 'r', encoding='utf-8') as f:\n",
    "                scidoc.raw_contents = f.read()\n",
    "                \n",
    "            ### Detect methods sections and extract relevant text from TXT\n",
    "            # here using regular expression for Orchards corpus, TXT\n",
    "            section_titles_txt, relevant_text_txt = gp.extract_methods_text(scidoc.raw_contents, \n",
    "                                                                            re_to_match=gp.RE_ORCHARDS_METHODS_TEXT)\n",
    "            print(\"Section titles text:\")\n",
    "            print(section_titles_txt)  # section titles in a list\n",
    "            print(\"Relevant content text, length: %s\" %len(relevant_text_txt))  # one single string\n",
    "        else:\n",
    "            print(\"No txt document for %s\" %filename_raw)\n",
    "            section_titles_txt = []\n",
    "            relevant_text_txt = ''\n",
    "            \n",
    "        ### PROCESS XML ###\n",
    "        filename_xml = filename_raw + du.XML_extension\n",
    "        filepath_xml = os.path.join(root,filename_xml)\n",
    "        if os.path.isfile(filepath_xml):\n",
    "            count_xml_files += 1\n",
    "            # defaults to False in class\n",
    "            scidoc.has_xml = True\n",
    "            tree = ET.parse(filepath_xml)\n",
    "            xml_root = tree.getroot()\n",
    "            scidoc.xml_root = xml_root\n",
    "            # Extract info directly from XML\n",
    "            scidoc.title = du.get_article_title(xml_root)\n",
    "            scidoc.year = du.get_publication_year(xml_root)\n",
    "            scidoc.journal = du.get_journal_title(xml_root)\n",
    "            scidoc.xml_contents = du.extract_content_text(xml_root)\n",
    "            scidoc.authors, scidoc.affiliations = du.get_article_authors_affiliations(xml_root)\n",
    "            scidoc.countries = du.get_affiliation_countries(xml_root)\n",
    "            \n",
    "            ### Detect methods sections and extract relevant text from XML\n",
    "            # here using regular expression for Orchards corpus, XML\n",
    "            methods_content_xml = gp.extract_methods_xml(scidoc.xml_root, re_to_match=gp.RE_ORCHARDS_METHODS_HEADINGS)\n",
    "            section_titles_xml = [item[0] for item in methods_content_xml]  # a list\n",
    "            relevant_text_list_xml = [item[1] for item in methods_content_xml]  # a list\n",
    "            relevant_text_xml = '\\n\\n'.join(txt for txt in relevant_text_list_xml)\n",
    "            print(\"Section titles xml:\")\n",
    "            print(section_titles_xml)\n",
    "            print(\"Relevant content xml, length: %s\" %len(relevant_text_xml))\n",
    "        else:\n",
    "            print(\"No xml document for %s\" %filename_raw)\n",
    "            section_titles_xml = []\n",
    "            relevant_text_xml = ''\n",
    "            \n",
    "        ### Continue with XML or TXT? ###\n",
    "        # use XML content unless:\n",
    "        #   - we found no relevant headings in XML, or\n",
    "        #   - we have insufficient content in the XML text portions\n",
    "        use_xml = True\n",
    "        if (len(section_titles_xml) == 0) or (len(relevant_text_xml) < min_characters):\n",
    "            use_xml = False\n",
    "\n",
    "        print(\"Using XML: %s\" %use_xml)\n",
    "        scidoc.use_xml = use_xml\n",
    "        section_titles = []\n",
    "        relevant_text = ''\n",
    "        if use_xml:\n",
    "            scidoc.methods_sections = section_titles_xml\n",
    "            scidoc.relevant_text = relevant_text_xml\n",
    "        else:\n",
    "            scidoc.methods_sections = section_titles_txt\n",
    "            scidoc.relevant_text = relevant_text_txt\n",
    "            \n",
    "print(\"We have %s ScienceDocs.\" %len(documents))\n",
    "print(\"We have %s pdf documents.\" %count_pdf_files)\n",
    "print(\"We have %s txt documents.\" %count_txt_files)\n",
    "print(\"We have %s xml documents.\" %count_xml_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickled data at science_articles.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# serialize the ScienceDocs for ease of use in the next step\n",
    "du.pickle_data(documents, path_to_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
